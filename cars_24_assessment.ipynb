{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a6b12df",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-necessary-libraries\" data-toc-modified-id=\"Import-necessary-libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import necessary libraries</a></span></li><li><span><a href=\"#Load-the-dataset\" data-toc-modified-id=\"Load-the-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load the dataset</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Predictions\" data-toc-modified-id=\"Predictions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Predictions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663c32f",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16240b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe31cce",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1bc3a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnsit_data = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a16699",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d14571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a simple model\n",
    "(x_train, y_train), (x_test, y_test) = mnsit_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c22d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing them to the range of (0-1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28834629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channel dimension for convolutional layer (reshape images)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c060538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e565638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.1826 - accuracy: 0.9455 - val_loss: 0.0593 - val_accuracy: 0.9801\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.0658 - accuracy: 0.9796 - val_loss: 0.0463 - val_accuracy: 0.9852\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0437 - accuracy: 0.9863 - val_loss: 0.0465 - val_accuracy: 0.9840\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.0331 - accuracy: 0.9896 - val_loss: 0.0380 - val_accuracy: 0.9882\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0251 - accuracy: 0.9919 - val_loss: 0.0449 - val_accuracy: 0.9872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c22168c040>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff72ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mnist_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529c491",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a702993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n",
      "Predicted Digit: 8\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model\n",
    "model = tf.keras.models.load_model(\"mnist_model.h5\")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(\"handwritten_digit.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Resize the image to 28x28 (same size as MNIST images)\n",
    "resized_image = cv2.resize(image, (28, 28))\n",
    "\n",
    "# Invert the image (if necessary, depending on how you saved it)\n",
    "inverted_image = cv2.bitwise_not(resized_image)\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "normalized_image = inverted_image / 255.0\n",
    "\n",
    "# Display the original and preprocessed images\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(normalized_image, cmap='gray')\n",
    "plt.title('Preprocessed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Reshape the image to match the model input shape and add batch dimension\n",
    "input_image = normalized_image.reshape(1, 28, 28, 1)\n",
    "\n",
    "# Perform prediction\n",
    "prediction = model.predict(input_image)\n",
    "\n",
    "# Get the predicted class (index of the highest probability)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "print(\"Predicted Digit:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005dc301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
